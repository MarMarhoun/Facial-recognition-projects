{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender classification using Convolutional Neural Network (ConvNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the tensorflow\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "# The tensorflow version in this project is : 2.7.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow import *\n",
    "print(tf.__version__)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load the dataset from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14000 files belonging to 2 classes.\n",
      "Using 11200 files for training.\n",
      "Found 14000 files belonging to 2 classes.\n",
      "Using 2800 files for validation.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "###### 1) Upload dataset from directories\n",
    "DATADIR = \"data/train/\"\n",
    "CATEGORIES = ['female', 'male']\n",
    "\n",
    "IMG_SIZE = 256\n",
    "batch_size= 32 # Others batch sizes 64, 128, and 256.\n",
    "\n",
    "ds_train_ = tf.keras.preprocessing.image_dataset_from_directory(DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"binary\", # int or categorical\n",
    "    class_names = CATEGORIES,\n",
    "    #color_mode =\"grayscale\", # Uncomment thi to wirk in the gray scale\n",
    "    batch_size = batch_size,\n",
    "    image_size = (IMG_SIZE,IMG_SIZE), # reshape if not in this size\n",
    "    shuffle = True,\n",
    "    seed= 123, # to maintain the same data when spliyying the data\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    ")\n",
    "\n",
    "\n",
    "ds_valid_ = tf.keras.preprocessing.image_dataset_from_directory(DATADIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"binary\", # Other label modes are: int or categorical\n",
    "    class_names = CATEGORIES,\n",
    "    #color_mode =\"grayscale\", # Uncomment thi to wirk in the gray scale\n",
    "    batch_size = batch_size,\n",
    "    image_size = (IMG_SIZE,IMG_SIZE), # reshape if not in this size\n",
    "    shuffle = True,\n",
    "    seed= 123, # to maintain the same results for different runs and data when splitting the data \n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Preprocess the data\n",
    "\n",
    "###### Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n",
    "\n",
    "+ **Dataset.cache** keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "+ **Dataset.prefetch** overlaps data preprocessing and model execution while training.\n",
    "\n",
    "> ***For more details, please visit***  [Tensorflow tutorials: classificattion](https://www.tensorflow.org/tutorials/images/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function convert_to_float at 0x0000021333C13F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function convert_to_float at 0x0000021333C13F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Search for this \n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Create the model\n",
    "\n",
    "The Sequential model consists of three basic convolution blocks (tf.keras.layers.Conv2D) with a max pooling layer (tf.keras.layers.MaxPooling2D) in each of them (the base) followed by a head of Dense layers. There's a fully-connected layer (tf.keras.layers.Dense) with 128 units on top of it that is activated by a ReLU activation function ('relu'). \n",
    "\n",
    "> *NP: This model has not been tuned for high accuracy.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 128, 128, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 64, 64, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 32, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16777344  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,872,257\n",
      "Trainable params: 16,872,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "IMG_SIZE = 256\n",
    "batch_size= 32 # Others batch sizes 64, 128, and 256.\n",
    "\n",
    "model = keras.Sequential([\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same',\n",
    "                  # give the input dimensions in the first layer\n",
    "                  # [height, width, color channels(RGB)]\n",
    "                  input_shape=[IMG_SIZE,IMG_SIZE, 3]), # Please recheck the number of channels in your input images\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=128, activation=\"relu\"),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with data augumentation\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[128, 128, 3]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    preprocessing.RandomContrast(factor=0.10),\n",
    "    preprocessing.RandomFlip(mode='horizontal'),\n",
    "    preprocessing.RandomRotation(factor=0.10),\n",
    "    \n",
    "    #preprocessing.RandomContrast(factor=0.3),\n",
    "   # preprocessing.RandomFlip(mode='horizontal'), # meaning, left-to-right\n",
    "    #preprocessing.RandomFlip(mode='vertical'), # meaning, top-to-bottom\n",
    "    #preprocessing.RandomWidth(factor=0.15), # horizontal stretch\n",
    "   # preprocessing.RandomRotation(factor=0.20),\n",
    "    #preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "\n",
    "    # Block One\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.BatchNormalization(renorm=True),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "\n",
    "# Compile and fit the model \n",
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "import pandas as pd\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Compile the model\n",
    "\n",
    "I choosed the tf.keras.optimizers.Adam optimizer and binary_crossentropy loss function. To view training and validation accuracy for each training epoch, pass the metrics argument to Model.compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model \n",
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) train the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000021333C7E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000021333C7E438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "350/350 [==============================] - ETA: 0s - loss: 5.0131 - binary_accuracy: 0.6610 WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000021333F92318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000021333F92318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "350/350 [==============================] - 6602s 19s/step - loss: 5.0131 - binary_accuracy: 0.6610 - val_loss: 0.5829 - val_binary_accuracy: 0.7157\n",
      "Epoch 2/50\n",
      "  4/350 [..............................] - ETA: 1:26:12 - loss: 0.5832 - binary_accuracy: 0.6484"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
